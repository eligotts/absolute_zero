import asyncio
import ast
import json
import logging
import random
import re
import inspect
import time
import os
from copy import deepcopy
from typing import Any, Dict, List, Optional, Tuple
from azr_utils.azr_parser import AZRXMLParser
from azr_utils.azr_executor import AZRExecutor
from azr_utils.azr_buffers import AZRBufferManager, Triplet, DeductionItem, AbductionItem, InductionItem
from datasets import Dataset
from openai import AsyncOpenAI

import verifiers as vf
from verifiers.envs.environment import Environment
from verifiers.rubrics.rubric import Rubric
from verifiers.types import ChatMessage, Info, Messages, SamplingArgs, State

from azr_utils.azr_prompts import (
    INSTRUCTION_FOLLOWING,
    BASE_SYSTEM_PROMPT,
    CODE_OUTPUT_PREDICTOR_PROMPT,
    CODE_INPUT_PREDICTOR_PROMPT,
    CODE_FUNCTION_PREDICTOR_PROMPT,
    PROPOSE_DEDUCTION_PROMPT,
    PROPOSE_ABDUCTION_PROMPT,
    PROPOSE_INDUCTION_PROMPT,
)


# ------------ Logging setup (file-based) ------------

AZR_LOG_FILE = os.getenv("AZR_LOG_FILE", "azr_runs.log")

def _ensure_run_logger() -> logging.Logger:
    logger = logging.getLogger("AZRRunLog")
    if not any(isinstance(h, logging.FileHandler) and getattr(h, "_azr_is_run_log", False) for h in logger.handlers):
        fh = logging.FileHandler(AZR_LOG_FILE, encoding="utf-8")
        fh.setLevel(logging.INFO)
        setattr(fh, "_azr_is_run_log", True)
        fmt = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
        fh.setFormatter(fmt)
        logger.addHandler(fh)
    logger.setLevel(logging.INFO)
    return logger


# =========================
# Rubric: composite AZR reward
# =========================

class AZRRubric(Rubric):
    def __init__(self, parser: Optional[AZRXMLParser] = None, executor: Optional[AZRExecutor] = None, exec_timeout: float = 20.0):
        self.azr_parser = parser or AZRXMLParser()
        self.executor = executor or AZRExecutor(max_exec_seconds=exec_timeout)
        super().__init__(funcs=[self.azr_reward], weights=[1.0], parser=self.azr_parser)
        self.run_logger = _ensure_run_logger()

    @staticmethod
    def _parse_literal_maybe_tuple(content: str) -> Tuple[bool, Optional[Any]]:
        s = content.strip()
        # Try JSON first (only if looks like JSON)
        if s.startswith("{") or s.startswith("[") or (s.startswith('"') and s.endswith('"')):
            try:
                return True, json.loads(s)
            except Exception:
                pass
        # Try Python literal
        try:
            return True, ast.literal_eval(s)
        except Exception:
            pass
        # Try wrapping in tuple for multi-arg without parens
        if "," in s and not s.startswith(("[", "{", "(", "'", '"')):
            try:
                return True, ast.literal_eval(f"({s})")
            except Exception:
                pass
        return False, None

    async def _mc_model_call(
        self,
        client: AsyncOpenAI,
        model: str,
        messages: List[ChatMessage],
        sampling_args: Optional[SamplingArgs],
        oai_tools: Optional[Any],
        message_type: str,
    ) -> Any:
        sampling_args = sampling_args or {}
        # Normalize sampling args (mirror Environment.get_model_response)
        args = dict(sampling_args)
        if "max_tokens" in args:
            if args["max_tokens"] is None:
                args.pop("max_tokens")
            elif message_type == "chat":
                args["max_completion_tokens"] = args.pop("max_tokens")
        if "max_completion_tokens" in args and args["max_completion_tokens"] is None:
            args.pop("max_completion_tokens")
        args = {k: v for k, v in args.items() if v is not None}
        assert message_type == "chat", "AZR currently supports chat MC calls only"
        if oai_tools:
            return await client.chat.completions.create(
                model=model,
                messages=messages,  # type: ignore
                tools=oai_tools,
                **args,
            )
        return await client.chat.completions.create(
            model=model,
            messages=messages,  # type: ignore
            **args,
        )

    def _build_ds_prompt_r(self, program: str, inp: Any) -> List[ChatMessage]:
        prompt_text = CODE_OUTPUT_PREDICTOR_PROMPT.format(snippet=program, input_args=inp)
        wrapped = INSTRUCTION_FOLLOWING.format(prompt_text)
        return [{"role": "user", "content": wrapped}]

    def _build_as_prompt_r(self, program: str, gold_out: Any) -> List[ChatMessage]:
        prompt_text = CODE_INPUT_PREDICTOR_PROMPT.format(snippet=program, output=gold_out)
        wrapped = INSTRUCTION_FOLLOWING.format(prompt_text)
        return [{"role": "user", "content": wrapped}]

    def _build_is_prompt_r(self, visible_pairs: List[Tuple[Any, Any]], message: str) -> List[ChatMessage]:
        pairs_str_parts = []
        for idx, (inp, out) in enumerate(visible_pairs):
            pairs_str_parts.append(f"```input_{idx}\n{inp}\n```\n```output_{idx}\n{out}\n```\n")
        pairs_block = "".join(pairs_str_parts)
        prompt_text = CODE_FUNCTION_PREDICTOR_PROMPT.format(input_output_pairs=pairs_block, message=message)
        wrapped = INSTRUCTION_FOLLOWING.format(prompt_text)
        return [{"role": "user", "content": wrapped}]

    async def _mc_deduction_solve_r(
        self,
        program: str,
        inp: Any,
        gold_out: Any,
        mc_samples: int,
        client: AsyncOpenAI,
        model: str,
        sampling_args: Optional[SamplingArgs],
        oai_tools: Optional[Any],
        message_type: str,
    ) -> float:
        messages = self._build_ds_prompt_r(program, inp)

        async def run_one(idx: int) -> int:
            resp = await self._mc_model_call(client, model, messages, sampling_args, oai_tools, message_type)
            txt = resp.choices[0].message.content or ""
            parsed_ok = False
            pred_out: Any = None
            blocks = self.azr_parser.parse_answer(txt, fences=["output"]) or {}
            outs = blocks.get("output", []) if isinstance(blocks, dict) else []
            if outs:
                ok, val = self._parse_literal_maybe_tuple(outs[0])
                if ok:
                    pred_out, parsed_ok = val, True
            try:
                self.run_logger.info(json.dumps({
                    "mc_task": "deduction.propose monte carlo rollout solver",
                    "sample_index": idx,
                    "format_ok": parsed_ok,
                    "prediction": pred_out,
                    "gold_output": gold_out,
                    "raw_response_preview": None if parsed_ok else txt[:1200],
                }))
            except Exception:
                pass
            return 1 if (parsed_ok and pred_out == gold_out) else 0

        if mc_samples <= 0:
            return 0.0
        results = await asyncio.gather(*[asyncio.create_task(run_one(i)) for i in range(mc_samples)])
        correct = sum(int(x) for x in results)
        return correct / float(mc_samples)

    async def _mc_abduction_solve_r(
        self,
        program: str,
        gold_out: Any,
        mc_samples: int,
        client: AsyncOpenAI,
        model: str,
        sampling_args: Optional[SamplingArgs],
        oai_tools: Optional[Any],
        message_type: str,
    ) -> float:
        messages = self._build_as_prompt_r(program, gold_out)

        async def run_one(idx: int) -> int:
            resp = await self._mc_model_call(client, model, messages, sampling_args, oai_tools, message_type)
            txt = resp.choices[0].message.content or ""
            parsed_ok = False
            pred_in: Any = None
            blocks = self.azr_parser.parse_answer(txt, fences=["input"]) or {}
            ins = blocks.get("input", []) if isinstance(blocks, dict) else []
            if ins:
                ok, val = self._parse_literal_maybe_tuple(ins[0])
                if ok:
                    pred_in, parsed_ok = val, True
            try:
                self.run_logger.info(json.dumps({
                    "mc_task": "abduction.propose monte carlo rollout solver",
                    "sample_index": idx,
                    "format_ok": parsed_ok,
                    "prediction": pred_in,
                    "program_preview": program[:120] if isinstance(program, str) else None,
                    "gold_output": gold_out,
                    "raw_response_preview": None if parsed_ok else txt[:1200],
                }))
            except Exception:
                pass
            ok = parsed_ok and self.executor.eval_abduction_input(
                code=program, gold_output=gold_out, agent_input=pred_in, runs=2
            )
            return 1 if ok else 0

        if mc_samples <= 0:
            return 0.0
        results = await asyncio.gather(*[asyncio.create_task(run_one(i)) for i in range(mc_samples)])
        correct = sum(int(x) for x in results)
        return correct / float(mc_samples)

    async def _mc_induction_solve_r(
        self,
        message: str,
        visible_pairs: List[Tuple[Any, Any]],
        hidden_pairs: List[Tuple[Any, Any]],
        mc_samples: int,
        client: AsyncOpenAI,
        model: str,
        sampling_args: Optional[SamplingArgs],
        oai_tools: Optional[Any],
        message_type: str,
    ) -> float:
        messages = self._build_is_prompt_r(visible_pairs, message)

        async def run_one(idx: int) -> int:
            resp = await self._mc_model_call(client, model, messages, sampling_args, oai_tools, message_type)
            txt = resp.choices[0].message.content or ""
            parsed_ok = False
            program_src: Optional[str] = None
            blocks = self.azr_parser.parse_answer(txt, fences=["python"]) or {}
            pys = blocks.get("python", []) if isinstance(blocks, dict) else []
            if pys:
                program_src = pys[0]
                parsed_ok = True
            try:
                self.run_logger.info(json.dumps({
                    "mc_task": "induction.propose monte carlo rollout solver",
                    "sample_index": idx,
                    "format_ok": parsed_ok,
                    "program_preview": program_src[:200] if isinstance(program_src, str) else None,
                    "hidden_pairs": hidden_pairs,
                    "raw_response_preview": None if parsed_ok else txt[:1200],
                }))
            except Exception:
                pass
            ok = isinstance(program_src, str) and self.executor.eval_program_on_pairs(
                code=program_src, io_pairs=hidden_pairs, runs=2
            )
            return 1 if ok else 0

        if mc_samples <= 0:
            return 0.0
        results = await asyncio.gather(*[asyncio.create_task(run_one(i)) for i in range(mc_samples)])
        correct = sum(int(x) for x in results)
        return correct / float(mc_samples)

    async def azr_reward(
        self,
        prompt: Messages,
        completion: Messages,
        answer: str,
        state: State,
        task: str = "default",
        info: Info | None = None,
        **kwargs,
    ) -> float:
        # Composite reward mapping with format-aware penalties.
        # Propose: r = 0 if mc in {0,1}; else 1 - mc. Invalid -> -0.5; format error -> -1.0
        # Solve:   r = 1.0 if correct else -0.5. Format error -> -1.0
        # Format gating
        format_ok = bool(state.get("format_ok", False))
        if not format_ok:
            return -1.0
        # Extract runtime context for MC evaluation when needed
        info = info or {}
        runtime = state.get("runtime", {}) or {}
        client = runtime.get("client")
        model = runtime.get("model")
        sampling_args = runtime.get("sampling_args")
        oai_tools = runtime.get("oai_tools")
        message_type = runtime.get("message_type", "chat")
        # Propose vs Solve
        if task.endswith(".propose"):
            # Ensure MC accuracy is computed here if missing
            valid = bool(state.get("valid", False))
            prop = state.get("propose", {}) or {}
            payload = state.get("payload", {}) or {}
            if not valid:
                return -0.5
            mc_acc = prop.get("mc_accuracy", None)
            mc_samples = int(info.get("mc_samples", prop.get("mc_samples", 0)))
            if mc_acc is None and mc_samples > 0:
                if task == "deduction.propose":
                    program = payload.get("program")
                    inp = payload.get("input")
                    gold_out = payload.get("output")
                    if isinstance(program, str):
                        mc_acc = await self._mc_deduction_solve_r(
                            program=program,
                            inp=inp,
                            gold_out=gold_out,
                            mc_samples=mc_samples,
                            client=client,
                            model=model,
                            sampling_args=sampling_args,
                            oai_tools=oai_tools,
                            message_type=message_type,
                        )
                elif task == "abduction.propose":
                    program = payload.get("program")
                    gold_out = payload.get("output")
                    if isinstance(program, str):
                        mc_acc = await self._mc_abduction_solve_r(
                            program=program,
                            gold_out=gold_out,
                            mc_samples=mc_samples,
                            client=client,
                            model=model,
                            sampling_args=sampling_args,
                            oai_tools=oai_tools,
                            message_type=message_type,
                        )
                elif task == "induction.propose":
                    message = payload.get("message")
                    visible_pairs = payload.get("visible_pairs") or []
                    hidden_pairs = payload.get("hidden_pairs") or []
                    mc_acc = await self._mc_induction_solve_r(
                        message=message,
                        visible_pairs=visible_pairs,
                        hidden_pairs=hidden_pairs,
                        mc_samples=mc_samples,
                        client=client,
                        model=model,
                        sampling_args=sampling_args,
                        oai_tools=oai_tools,
                        message_type=message_type,
                    )
                # Persist MC result back into state for downstream visibility
                state.setdefault("propose", {}).update({"mc_accuracy": mc_acc})
            # Compute reward mapping
            mc = state.get("propose", {}).get("mc_accuracy", None)
            if mc is None:
                return -0.5
            try:
                mc_f = float(mc)
            except Exception:
                return -0.5
            if mc_f <= 0.0 or mc_f >= 1.0:
                return 0.0
            return 1.0 - mc_f
        else:
            # Ensure correctness is computed here if missing
            solve = state.get("solve", {}) or {}
            correct = solve.get("correct", None)
            payload = state.get("payload", {}) or {}
            if correct is None:
                if task == "deduction.solve":
                    pred_out = payload.get("output")
                    gold_out = payload.get("gold_output")
                    correct = (pred_out == gold_out)
                elif task == "abduction.solve":
                    program = payload.get("program")
                    pred_input = payload.get("input")
                    gold_out = payload.get("gold_output")
                    if isinstance(program, str):
                        correct = self.executor.eval_abduction_input(
                            code=program,
                            gold_output=gold_out,
                            agent_input=pred_input,
                            runs=2,
                        )
                    else:
                        correct = False
                elif task == "induction.solve":
                    program = payload.get("program")
                    hidden_pairs = payload.get("hidden_pairs") or []
                    if isinstance(program, str):
                        correct = self.executor.eval_program_on_pairs(
                            code=program,
                            io_pairs=hidden_pairs,
                            runs=2,
                        )
                    else:
                        correct = False
                else:
                    correct = False
                # Persist
                state.setdefault("solve", {}).update({"correct": bool(correct)})
            return 1.0 if bool(state.get("solve", {}).get("correct", False)) else -0.5


# =========================
# Environment
# =========================

class AZREnv(Environment):
    """
    AZR Environment implementing six task-role configurations:
      deduction.propose, abduction.propose, induction.propose,
      deduction.solve,   abduction.solve,   induction.solve
    """

    def __init__(
        self,
        dataset: Dataset,
        system_prompt: str = "",
        mc_samples: int = 8,
        proposer_K: int = 6,
        N: int = 10,
        determinism_runs: int = 2,
        seed: int = 1337420,
        init_zero_triplet: bool = True,
        verbose: bool = False,
        exec_timeout: float = 20.0,
        **kwargs,
    ):
        self.logger = logging.getLogger("AZREnv")
        self.azr_parser = AZRXMLParser()
        self.executor = AZRExecutor(max_exec_seconds=exec_timeout)
        self.rubric_impl = AZRRubric(parser=self.azr_parser, executor=self.executor, exec_timeout=exec_timeout)
        super().__init__(
            dataset=dataset,
            system_prompt=None,  # prompts are pre-formatted with system message
            parser=self.azr_parser,
            rubric=self.rubric_impl,
            message_type="chat",
            **kwargs,
        )
        random.seed(seed)
        self.buffers = AZRBufferManager(seed=seed, init_zero_triplet=init_zero_triplet)
        # number of monte carlo samples to use for propose tasks
        self.mc_samples = mc_samples
        # number of references to use for propose tasks
        self.K = proposer_K
        # number of inputs to ask for in induction propose tasks
        self.N = N
        self.j = determinism_runs
        self.seed = seed
        self.verbose = verbose

        # --- Auto-seeding coordination primitives ---
        # Rollouts may start before buffers are seeded. We coordinate a single
        # real seeding run, while concurrent rollouts wait until seeding completes.
        # Seeding itself uses rollout(), so we also support an internal bypass flag
        # to avoid recursion/deadlock.
        self._seeded: bool = False
        self._seed_in_progress: bool = False
        # asyncio primitives are safe to construct here; they are bound to the
        # running loop when awaited/used.
        self._seed_lock: asyncio.Lock = asyncio.Lock()
        self._seed_event: asyncio.Event = asyncio.Event()

    # ------------- Prompt builders -------------

    def _build_snippet_blocks(self, refs: List[Triplet], problem_type: str) -> str:
        # Build reference blocks exactly like AZR (<snippet_i> with python/input/output fences)
        out_key = "output" if problem_type != "code_e" else "error"
        blocks: List[str] = []
        for i, t in enumerate(refs):
            blocks.append(
                f"<snippet_{i}>\n```python\n{t.program}\n```\n```input\n{t.input}\n```\n```{out_key}\n{t.output}\n```\n</snippet_{i}>\n"
            )
        return "".join(blocks)

    def _render_banned_keywords(self) -> List[str]:
        # Derive banned keywords list (best-effort mapping of our safety policy)
        return [
            "os", "sys", "shutil", "subprocess", "pathlib", "socket", "requests", "urllib", "http",
            "jsonpickle", "pickle", "marshal", "ctypes", "importlib", "threading", "multiprocessing",
            "random", "numpy", "pandas", "builtins", "__import__", "eval", "exec", "open", "compile",
            "globals", "locals", "vars", "setattr", "delattr", "getattr",
        ]

    def _render_references(self, K: int, problem_type: str) -> str:
        refs = self.buffers.get_references(K)
        if not refs:
            return ""
        return self._build_snippet_blocks(refs, problem_type)

    def _build_dp_prompt(self, K: int) -> ChatMessage:
        # Proposer for deduction -> gen_code_o prompt (verbatim from AZR)
        snippet_blocks = self._render_references(K, problem_type="code_o")
        banned = ", ".join(self._render_banned_keywords())
        filled = PROPOSE_DEDUCTION_PROMPT.replace("<|BANNED_KEYWORDS|>", banned) + "\n" + snippet_blocks
        wrapped = INSTRUCTION_FOLLOWING.format(filled)
        return {"role": "user", "content": wrapped}

    def _build_ap_prompt(self, K: int) -> ChatMessage:
        # Proposer for abduction -> gen_code_i prompt (verbatim from AZR)
        snippet_blocks = self._render_references(K, problem_type="code_i")
        banned = ", ".join(self._render_banned_keywords())
        filled = PROPOSE_ABDUCTION_PROMPT.replace("<|BANNED_KEYWORDS|>", banned) + "\n" + snippet_blocks
        wrapped = INSTRUCTION_FOLLOWING.format(filled)
        return {"role": "user", "content": wrapped}

    def _build_ip_prompt(self, program: str, N: int) -> ChatMessage:
        # Proposer for induction -> gen_code_f prompt (verbatim from AZR)
        code_suffix = "\nf(<|YOUR INPUT WILL BE PLUGGED HERE|>)"
        filled_prompt = PROPOSE_INDUCTION_PROMPT.format(num_inputs=N, snippet=(program + code_suffix))
        wrapped = INSTRUCTION_FOLLOWING.format(filled_prompt)
        return {"role": "user", "content": wrapped}

    def _build_ds_prompt(self, item: DeductionItem) -> List[ChatMessage]:
        # Solver for deduction -> pred_code_o prompt (verbatim from AZR)
        prompt_text = CODE_OUTPUT_PREDICTOR_PROMPT.format(snippet=item.program, input_args=item.input)
        wrapped = INSTRUCTION_FOLLOWING.format(prompt_text)
        return [{"role": "user", "content": wrapped}]

    def _build_as_prompt(self, item: AbductionItem) -> List[ChatMessage]:
        # Solver for abduction -> pred_code_i prompt (verbatim from AZR)
        prompt_text = CODE_INPUT_PREDICTOR_PROMPT.format(snippet=item.program, output=item.output)
        wrapped = INSTRUCTION_FOLLOWING.format(prompt_text)
        return [{"role": "user", "content": wrapped}]

    def _build_is_prompt(self, item: InductionItem) -> List[ChatMessage]:
        # Solver for induction -> pred_code_f prompt (verbatim from AZR)
        pairs_str_parts = []
        for idx, (inp, out) in enumerate(item.visible_pairs):
            pairs_str_parts.append(f"```input_{idx}\n{inp}\n```\n```output_{idx}\n{out}\n```\n")
        pairs_block = "".join(pairs_str_parts)
        prompt_text = CODE_FUNCTION_PREDICTOR_PROMPT.format(input_output_pairs=pairs_block, message=item.message)
        wrapped = INSTRUCTION_FOLLOWING.format(prompt_text)
        return [{"role": "user", "content": wrapped}]

    # ------------- Rollout -------------

    async def rollout(
        self,
        client: AsyncOpenAI,
        model: str,
        prompt: Messages,
        answer: str = "",
        task: str = "default",
        info: Info | None = None,
        sampling_args: SamplingArgs | None = None,
        **kwargs,
    ) -> tuple[Messages, State]:
        info = info or {}
        K = int(info.get("K", self.K))
        N = int(info.get("N", self.N))
        mc = int(info.get("mc_samples", self.mc_samples))
        j = int(info.get("determinism_runs", self.j))
        oai_tools = info.get("oai_tools", None)

        # Auto-seed guard: ensure buffers are populated before proceeding,
        bypass_auto_seed = bool(info.get("_bypass_seeding", False))
        if not bypass_auto_seed:
            await self._ensure_seeded_if_needed(
                client=client,
                model=model,
                sampling_args=sampling_args,
                oai_tools=oai_tools,
            )

        # Build dynamic messages using AZR-style instruction-following wrappers
        rollout_messages: List[ChatMessage] = []
        if isinstance(prompt, list):
            rollout_messages = deepcopy(prompt)
        else:
            rollout_messages = [{"role": "user", "content": str(prompt)}]

        # Pre-sample items when needed (prior to model call)
        sampled = None
        induction_context_program: Optional[str] = None
        if task == "deduction.propose":
            rollout_messages.append(self._build_dp_prompt(K))
        elif task == "abduction.propose":
            rollout_messages.append(self._build_ap_prompt(K))
        elif task == "induction.propose":
            # For induction, choose a recent program to condition on
            induction_context_program = self.buffers.sample_program_from_union()
            if induction_context_program is None:
                # Fallback: take latest triplet program
                refs = self.buffers.get_references(1)
                induction_context_program = refs[0].program if refs else "def f(x):\n    return x"
            rollout_messages.append(self._build_ip_prompt(induction_context_program, N))
        elif task == "deduction.solve":
            # Prefer recent validated items; fallback to seed
            sampled = self.buffers.sample_deduction()
            if sampled is None:
                # Seed fallback
                refs = self.buffers.get_references(1)
                if refs:
                    t = refs[0]
                    sampled = DeductionItem(program=t.program, input=t.input, output=t.output, step_id=t.step_id, created_at=t.created_at)
                else:
                    # absolute fallback to zero
                    sampled = DeductionItem(program="def f(x):\n    return x", input="Hello World", output="Hello World", step_id=0, created_at=time.time())
            rollout_messages = self._build_ds_prompt(sampled)
        elif task == "abduction.solve":
            sampled = self.buffers.sample_abduction()
            if sampled is None:
                refs = self.buffers.get_references(1)
                if refs:
                    t = refs[0]
                    sampled = AbductionItem(program=t.program, input=t.input, output=t.output, step_id=t.step_id, created_at=t.created_at)
                else:
                    sampled = AbductionItem(program="def f(x):\n    return x", input="Hello World", output="Hello World", step_id=0, created_at=time.time())
            rollout_messages = self._build_as_prompt(sampled)
        elif task == "induction.solve":
            sampled = self.buffers.sample_induction()
            if sampled is None:
                # Bootstrap from zero triplet if needed
                refs = self.buffers.get_references(1)
                prog = refs[0].program if refs else "def f(x):\n    return x"
                # create trivial visible/hidden pairs
                pairs = [("A", "A"), ("B", "B")]
                vis = pairs[:1]
                hid = pairs[1:]
                self.buffers.add_induction(program=prog, message="Return the input unchanged.", io_pairs=pairs, visible=vis, hidden=hid)
                sampled = self.buffers.sample_induction()
            assert sampled is not None
            rollout_messages = self._build_is_prompt(sampled)
        else:
            rollout_messages.append({"role": "user", "content": "Continue"})

        # Print prompt being sent to model
        if self.verbose:
            try:
                print(f"\n[PROMPT TO MODEL] task={task}")
                print("=" * 60)
                for i, msg in enumerate(rollout_messages):
                    role = msg.get("role", "unknown")
                    content = msg.get("content", "")
                    print(f"Message {i+1} ({role}):")
                    print(content)
                    print("-" * 40)
                print("=" * 60)
                print(f"[END PROMPT]\n")
            except Exception:
                pass

        # Call model for main role
        run_logger = _ensure_run_logger()
        is_seeding_phase = bool(info.get("_bypass_seeding", False))
        try:
            if is_seeding_phase:
                phase = "rollout_request_seeding"
            else:
                phase = "rollout_request"
            run_logger.info(json.dumps({
                "phase": phase,
                "task": task,
                "messages": rollout_messages,
                "info": info,
                "is_seeding": is_seeding_phase,
            }))
        except Exception:
            pass
        response = await self.get_model_response(
            client=client,
            model=model,
            prompt=rollout_messages,
            oai_tools=oai_tools,
            sampling_args=sampling_args,
            message_type=self.message_type,
        )
        assistant_text = response.choices[0].message.content or ""
        completion: Messages = [{"role": "assistant", "content": assistant_text}]
        try:
            if is_seeding_phase:
                phase = "rollout_response_seeding"
            else:
                phase = "rollout_response"
            run_logger.info(json.dumps({
                "phase": phase,
                "task": task,
                "assistant_text": assistant_text[:4000],
                "is_seeding": is_seeding_phase,
            }))
        except Exception:
            pass

        # Initialize state
        state: State = {
            "prompt": rollout_messages,
            "completion": completion,
            "answer": answer,
            "task": task,
            "info": info,
            "responses": [response],
            "turn": 1,
            # AZR-specific flags populated below
            "runtime": {
                "client": client,
                "model": model,
                "sampling_args": sampling_args,
                "oai_tools": oai_tools,
                "message_type": self.message_type,
            },
            "timing": {
                "generation_ms": 0.0,
                "scoring_ms": 0.0,
                "total_ms": 0.0,
            },
        }

        # Parse answer based on fenced-block formats (no JSON expected)
        json_obj = None
        parse_err = None

        try:
            if task == "deduction.propose" or task == "abduction.propose":
                blocks = self.azr_parser.parse_answer(assistant_text, fences=["python", "input"]) or {}
                pys = blocks.get("python", []) if isinstance(blocks, dict) else []
                ins = blocks.get("input", []) if isinstance(blocks, dict) else []
                if pys and ins:
                    ok, in_val = self.rubric_impl._parse_literal_maybe_tuple(ins[0])
                    in_parsed = in_val if ok else ins[0]
                    json_obj = {"program": pys[0], "input": in_parsed}
            elif task == "induction.propose":
                blocks = self.azr_parser.parse_answer(assistant_text, fences=["message", "input"]) or {}
                msgs = blocks.get("message", []) if isinstance(blocks, dict) else []
                ins = blocks.get("input", []) if isinstance(blocks, dict) else []
                if msgs and ins:
                    json_obj = {"message": msgs[0], "inputs": ins}
            elif task == "deduction.solve":
                blocks = self.azr_parser.parse_answer(assistant_text, fences=["output"]) or {}
                outs = blocks.get("output", []) if isinstance(blocks, dict) else []
                if outs:
                    ok, out_val = self.rubric_impl._parse_literal_maybe_tuple(outs[0])
                    json_obj = {"output": out_val if ok else outs[0]}
            elif task == "abduction.solve":
                blocks = self.azr_parser.parse_answer(assistant_text, fences=["input"]) or {}
                ins = blocks.get("input", []) if isinstance(blocks, dict) else []
                if ins:
                    ok, in_val = self.rubric_impl._parse_literal_maybe_tuple(ins[0])
                    json_obj = {"input": in_val if ok else ins[0]}
            elif task == "induction.solve":
                blocks = self.azr_parser.parse_answer(assistant_text, fences=["python"]) or {}
                pys = blocks.get("python", []) if isinstance(blocks, dict) else []
                if pys:
                    json_obj = {"program": pys[0]}
            else:
                # For unsupported tasks, leave json_obj None
                pass
        except Exception as e:
            parse_err = f"fenced-parse error: {e}"

        state["format_ok"] = json_obj is not None
        state["json_ok"] = json_obj is not None
        state["error"] = parse_err
        # Log parsing exceptions to azr_runs.log
        if parse_err:
            try:
                run_logger = _ensure_run_logger()
                run_logger.error(json.dumps({
                    "phase": "rollout_parse_error",
                    "task": task,
                    "error": parse_err,
                    "assistant_text_preview": assistant_text[:800],
                    "is_seeding": is_seeding_phase,
                }))
            except Exception:
                pass
        # Diagnostics for rollout formatting
        try:
            self.logger.info(
                f"[ROLL] task={task} format_ok={state['format_ok']} json_ok={state['json_ok']} error={(parse_err or '')[:80]}"
            )
            # Print raw model response for debugging
            if self.verbose:
                print(f"\n[RAW MODEL RESPONSE] task={task}")
                print(f"Response length: {len(assistant_text)} chars")
                print(f"Response content:\n{assistant_text}")
                print(f"[END RAW RESPONSE]\n")
        except Exception:
            pass
        state["payload"] = {
            "program": None,
            "input": None,
            "output": None,
            "message": None,
            "io_pairs": None,
            "visible_pairs": None,
            "hidden_pairs": None,
        }
        state["sampled_problem_id"] = None

        # Short-circuit on formatting failure for consistent penalties in rubric
        if not state["format_ok"] or json_obj is None:
            # valid=False for propose, solve correctness False
            if task.endswith(".propose"):
                state["valid"] = False
                state["propose"] = {"mc_samples": mc, "mc_accuracy": None}
                state["solve"] = None
            else:
                state["valid"] = False
                state["propose"] = None
                state["solve"] = {"correct": False}
            return completion, state

        # Branch by task for validation and rewards metadata
        if task == "deduction.propose":
            valid, prop_state, payload = await self._handle_dp(json_obj, j, mc, client, model, sampling_args, oai_tools)
            state["valid"] = valid
            state["propose"] = prop_state
            state["solve"] = None
            state["payload"] = payload
            if not valid and isinstance(prop_state, dict) and prop_state.get("error"):
                state["error"] = prop_state.get("error")
        elif task == "abduction.propose":
            valid, prop_state, payload = await self._handle_ap(json_obj, j, mc, client, model, sampling_args, oai_tools)
            state["valid"] = valid
            state["propose"] = prop_state
            state["solve"] = None
            state["payload"] = payload
            if not valid and isinstance(prop_state, dict) and prop_state.get("error"):
                state["error"] = prop_state.get("error")
        elif task == "induction.propose":
            valid, prop_state, payload = await self._handle_ip(json_obj, induction_context_program or "", j, mc, client, model, sampling_args, oai_tools)
            state["valid"] = valid
            state["propose"] = prop_state
            state["solve"] = None
            state["payload"] = payload
            if not valid and isinstance(prop_state, dict) and prop_state.get("error"):
                state["error"] = prop_state.get("error")
        elif task == "deduction.solve":
            payload = await self._handle_ds(json_obj, sampled, j, assistant_text)
            state["valid"] = True
            state["propose"] = None
            state["solve"] = {}
            state["payload"] = payload
            state["sampled_problem_id"] = getattr(sampled, "step_id", None)
        elif task == "abduction.solve":
            payload = await self._handle_as(json_obj, sampled, j, assistant_text)
            state["valid"] = True
            state["propose"] = None
            state["solve"] = {}
            state["payload"] = payload
            state["sampled_problem_id"] = getattr(sampled, "step_id", None)
        elif task == "induction.solve":
            payload = await self._handle_is(json_obj, sampled, j, assistant_text)
            state["valid"] = True
            state["propose"] = None
            state["solve"] = {}
            state["payload"] = payload
            state["sampled_problem_id"] = getattr(sampled, "step_id", None)
        else:
            state["valid"] = False
            state["propose"] = None
            state["solve"] = {"correct": False}

        return completion, state

    # ------------- Seeding (Buffers) -------------

    async def seed_buffers(
        self,
        client: AsyncOpenAI,
        model: str,
        *,
        target_triplets: Optional[int] = None,
        target_induction: Optional[int] = None,
        sampling_args: Optional[SamplingArgs] = None,
        oai_tools: Optional[Any] = None,
        max_attempts_per_stage: int = 20,
        parallel_chunk_size: int = 10,
    ) -> None:
        """
        Populate buffers by generating valid triplets, then induction items, using propose rollouts.

        - First, generate (program, input) â†’ output triplets via deduction/abduction propose tasks until
          triplet buffer size reaches target_triplets.
        - Then, generate induction items (program, message, io_pairs split into visible/hidden) until
          induction buffer size reaches target_induction.

        This mirrors the Absolute-Zero seeding approach at a high level, adapted to this environment.
        """
        # Determine targets
        desired_triplets = int(target_triplets) if target_triplets is not None else max(4 * self.K, 32)
        desired_induction = int(target_induction) if target_induction is not None else desired_triplets

        # Common info for rollouts
        info_common: Info = {
            "K": self.K,
            "N": self.N,
            "mc_samples": self.mc_samples,
            "determinism_runs": self.j,
            "oai_tools": oai_tools,
            # Internal flag so seed rollouts do not re-enter auto-seeding guard
            "_bypass_seeding": True,
        }

        # Seeding summary header
        try:
            self.logger.info(
                f"[SEED] start triplets={len(self.buffers.triplet_set)}->{desired_triplets} induction={len(self.buffers.induction)}->{desired_induction} K={self.K} mc={self.mc_samples} j={self.j}"
            )
        except Exception:
            pass

        # Stage 1: Seed triplets using propose tasks in parallel chunks
        need_triplets = max(0, desired_triplets - len(self.buffers.triplet_set))
        if need_triplets > 0:
            async def propose_triplet_once(task_type: str) -> bool:
                try:
                    _completion, state = await self.rollout(
                        client=client,
                        model=model,
                        prompt=[],
                        task=task_type,
                        info=info_common,
                        sampling_args=sampling_args,
                    )
                    return bool(state.get("valid", False))
                except Exception:
                    return False

            task_cycle = ["deduction.propose", "abduction.propose"]
            cycle_idx = 0
            successes_total = 0
            extra_budget = max_attempts_per_stage
            remaining = need_triplets
            while remaining > 0:
                chunk_target = min(remaining, max(1, int(parallel_chunk_size)))
                inflight: set[asyncio.Task] = set()
                for _ in range(chunk_target):
                    ttype = task_cycle[cycle_idx % len(task_cycle)]
                    cycle_idx += 1
                    inflight.add(asyncio.create_task(propose_triplet_once(ttype)))

                successes = 0
                while inflight and successes < chunk_target:
                    done, inflight = await asyncio.wait(inflight, return_when=asyncio.FIRST_COMPLETED)
                    for d in done:
                        ok = False
                        try:
                            ok = bool(await d)
                        except Exception:
                            ok = False
                        if ok:
                            successes += 1
                            successes_total += 1
                        else:
                            if extra_budget > 0 and (successes < chunk_target) and (successes_total < need_triplets):
                                extra_budget -= 1
                                ttype = task_cycle[cycle_idx % len(task_cycle)]
                                cycle_idx += 1
                                inflight.add(asyncio.create_task(propose_triplet_once(ttype)))

                    # Stop early if overall target reached
                    if successes_total >= need_triplets:
                        break

                # Cancel any remaining in this chunk when chunk target or overall target satisfied
                if inflight:
                    for t in inflight:
                        t.cancel()
                    try:
                        await asyncio.gather(*inflight, return_exceptions=True)
                    except Exception:
                        pass

                if successes_total >= need_triplets:
                    break
                # Reduce remaining by the number of successes achieved in this chunk
                remaining = need_triplets - successes_total

        # Safety clamp: do not exceed desired_triplets (best-effort; parallelism might be cancelled mid-flight)
        # No-op if already within bounds; buffers are append-only so we cannot remove.

        # Stage 2: Seed induction items in parallel chunks
        need_ind = max(0, desired_induction - len(self.buffers.induction))
        if need_ind > 0:
            async def propose_induction_once() -> bool:
                try:
                    _completion, state = await self.rollout(
                        client=client,
                        model=model,
                        prompt=[],
                        task="induction.propose",
                        info=info_common,
                        sampling_args=sampling_args,
                    )
                    return bool(state.get("valid", False))
                except Exception:
                    return False

            successes_total = 0
            extra_budget = max_attempts_per_stage
            remaining = need_ind
            while remaining > 0:
                chunk_target = min(remaining, max(1, int(parallel_chunk_size)))
                inflight: set[asyncio.Task] = set(asyncio.create_task(propose_induction_once()) for _ in range(chunk_target))
                successes = 0
                while inflight and successes < chunk_target:
                    done, inflight = await asyncio.wait(inflight, return_when=asyncio.FIRST_COMPLETED)
                    for d in done:
                        ok = False
                        try:
                            ok = bool(await d)
                        except Exception:
                            ok = False
                        if ok:
                            successes += 1
                            successes_total += 1
                        else:
                            if extra_budget > 0 and (successes < chunk_target) and (successes_total < need_ind):
                                extra_budget -= 1
                                inflight.add(asyncio.create_task(propose_induction_once()))
                    if successes_total >= need_ind:
                        break
                if inflight:
                    for t in inflight:
                        t.cancel()
                    try:
                        await asyncio.gather(*inflight, return_exceptions=True)
                    except Exception:
                        pass
                if successes_total >= need_ind:
                    break
                remaining = need_ind - successes_total

        try:
            self.logger.info(
                f"[SEED DONE] triplets={len(self.buffers.triplet_set)} induction={len(self.buffers.induction)}"
            )
        except Exception:
            pass

    # -------- Auto-seeding helpers --------

    def _desired_seed_targets(self, target_triplets: Optional[int] = None, target_induction: Optional[int] = None) -> tuple[int, int]:
        desired_triplets = int(target_triplets) if target_triplets is not None else min(4 * self.K, 4)
        desired_induction = int(target_induction) if target_induction is not None else desired_triplets
        return desired_triplets, desired_induction

    def _buffers_meet_targets(self, triplets_needed: int, induction_needed: int) -> bool:
        return len(self.buffers.triplet_set) >= triplets_needed and len(self.buffers.induction) >= induction_needed

    async def _ensure_seeded_if_needed(
        self,
        *,
        client: AsyncOpenAI,
        model: str,
        sampling_args: Optional[SamplingArgs],
        oai_tools: Optional[Any],
        target_triplets: Optional[int] = None,
        target_induction: Optional[int] = None,
    ) -> None:
        """
        Ensure buffers are seeded to target sizes. Exactly one rollout caller performs
        seeding while others wait. Seeding uses rollout() internally with a bypass flag.
        """
        triplet_goal, induction_goal = self._desired_seed_targets(target_triplets, target_induction)

        # Fast path if already sufficient
        if self._buffers_meet_targets(triplet_goal, induction_goal):
            self._seeded = True
            # Wake any stale waiters
            try:
                self._seed_event.set()
            except Exception:
                pass
            return

        # Multi-attempt loop: wait while another task seeds; otherwise seed.
        while True:
            if self._buffers_meet_targets(triplet_goal, induction_goal):
                self._seeded = True
                try:
                    self._seed_event.set()
                except Exception:
                    pass
                return

            if self._seed_in_progress:
                # Wait until current seeding attempt completes, then re-check.
                await self._seed_event.wait()
                # Loop to re-check targets; may start next attempt if still unmet.
                continue

            # Become the seeding leader
            async with self._seed_lock:
                # Double-check after acquiring lock
                if self._buffers_meet_targets(triplet_goal, induction_goal):
                    self._seeded = True
                    try:
                        self._seed_event.set()
                    except Exception:
                        pass
                    return
                # Mark in-progress and clear event for waiters
                self._seed_in_progress = True
                try:
                    self._seed_event.clear()
                except Exception:
                    pass

            # Perform seeding outside the lock (so rollouts called by seeding can run)
            try:
                self.logger.info(
                    f"[AUTO-SEED] starting: have triplets={len(self.buffers.triplet_set)} induction={len(self.buffers.induction)} target=({triplet_goal},{induction_goal})"
                )
            except Exception:
                pass
            try:
                await self.seed_buffers(
                    client=client,
                    model=model,
                    target_triplets=triplet_goal,
                    target_induction=induction_goal,
                    sampling_args=sampling_args,
                    oai_tools=oai_tools,
                )
            finally:
                # Finish attempt, wake waiters; if unmet, a waiter will loop and try again.
                self._seed_in_progress = False
                try:
                    self._seed_event.set()
                except Exception:
                    pass
                # Re-loop to either exit (targets met) or trigger another attempt.


    # -------- Propose handlers --------

    async def _handle_dp(
        self,
        json_obj: Dict[str, Any],
        determinism_runs: int,
        mc_samples: int,
        client: AsyncOpenAI,
        model: str,
        sampling_args: Optional[SamplingArgs],
        oai_tools: Optional[Any],
    ) -> Tuple[bool, Dict[str, Any], Dict[str, Any]]:
        program = json_obj.get("program", None)
        inp = json_obj.get("input", None)
        payload = {"program": program, "input": inp, "output": None, "message": None, "io_pairs": None, "visible_pairs": None, "hidden_pairs": None}
        if not isinstance(program, str):
            return False, {"mc_samples": mc_samples, "mc_accuracy": None, "error": "Program is not a string"}, payload
        # Validate program + input => output
        f, err = self.executor.compile_program(program)
        if f is None:
            try:
                self.logger.info(f"[PROPOSE FAIL] deduction.propose compile error: {err}")
            except Exception:
                pass
            # Also log error to azr_runs.log
            try:
                run_logger = _ensure_run_logger()
                run_logger.error(json.dumps({
                    "phase": "propose_compile_error",
                    "task": "deduction.propose",
                    "error": err,
                    "program_preview": (program[:200] if isinstance(program, str) else None),
                }))
            except Exception:
                pass
            return False, {"mc_samples": mc_samples, "mc_accuracy": None, "error": err}, payload
        ok, out, err2 = self.executor.run_deterministic(f, inp, runs=determinism_runs)
        if not ok:
            try:
                self.logger.info(f"[PROPOSE FAIL] deduction.propose execution error: {err2}")
            except Exception:
                pass
            # Also log error to azr_runs.log
            try:
                run_logger = _ensure_run_logger()
                run_logger.error(json.dumps({
                    "phase": "propose_exec_error",
                    "task": "deduction.propose",
                    "error": err2,
                    "program_preview": (program[:200] if isinstance(program, str) else None),
                    "input_preview": repr(inp)[:120],
                }))
            except Exception:
                pass
            return False, {"mc_samples": mc_samples, "mc_accuracy": None, "error": err2}, payload
        payload["output"] = out
        # Add to buffers
        step_id = self.buffers.add_triplet(program, inp, out)
        try:
            prog_preview = (program[:60] + "â€¦") if isinstance(program, str) and len(program or "") > 60 else program
            self.logger.info(
                f"[PROPOSE OK] deduction.propose step_id={step_id} input={repr(inp)[:40]} output={repr(out)[:40]} program_preview={repr(prog_preview)} total_triplets={len(self.buffers.triplet_set)}"
            )
        except Exception:
            pass
        # Defer MC solver accuracy to rubric stage
        return True, {"mc_samples": mc_samples, "mc_accuracy": None}, payload

    async def _handle_ap(
        self,
        json_obj: Dict[str, Any],
        determinism_runs: int,
        mc_samples: int,
        client: AsyncOpenAI,
        model: str,
        sampling_args: Optional[SamplingArgs],
        oai_tools: Optional[Any],
    ) -> Tuple[bool, Dict[str, Any], Dict[str, Any]]:
        program = json_obj.get("program", None)
        inp = json_obj.get("input", None)
        payload = {"program": program, "input": inp, "output": None, "message": None, "io_pairs": None, "visible_pairs": None, "hidden_pairs": None}
        if not isinstance(program, str):
            return False, {"mc_samples": mc_samples, "mc_accuracy": None, "error": "Program is not a string"}, payload
        f, err = self.executor.compile_program(program)
        if f is None:
            try:
                self.logger.info(f"[PROPOSE FAIL] abduction.propose compile error: {err}")
            except Exception:
                pass
            # Also log error to azr_runs.log
            try:
                run_logger = _ensure_run_logger()
                run_logger.error(json.dumps({
                    "phase": "propose_compile_error",
                    "task": "abduction.propose",
                    "error": err,
                    "program_preview": (program[:200] if isinstance(program, str) else None),
                }))
            except Exception:
                pass
            return False, {"mc_samples": mc_samples, "mc_accuracy": None, "error": err}, payload
        ok, out, err2 = self.executor.run_deterministic(f, inp, runs=determinism_runs)
        if not ok:
            try:
                self.logger.info(f"[PROPOSE FAIL] abduction.propose execution error: {err2}")
            except Exception:
                pass
            # Also log error to azr_runs.log
            try:
                run_logger = _ensure_run_logger()
                run_logger.error(json.dumps({
                    "phase": "propose_exec_error",
                    "task": "abduction.propose",
                    "error": err2,
                    "program_preview": (program[:200] if isinstance(program, str) else None),
                    "input_preview": repr(inp)[:120],
                }))
            except Exception:
                pass
            return False, {"mc_samples": mc_samples, "mc_accuracy": None, "error": err2}, payload
        payload["output"] = out
        step_id = self.buffers.add_triplet(program, inp, out)
        try:
            prog_preview = (program[:60] + "â€¦") if isinstance(program, str) and len(program or "") > 60 else program
            self.logger.info(
                f"[PROPOSE OK] abduction.propose step_id={step_id} input={repr(inp)[:40]} output={repr(out)[:40]} program_preview={repr(prog_preview)} total_triplets={len(self.buffers.triplet_set)}"
            )
        except Exception:
            pass
        # Defer MC solver accuracy to rubric stage
        return True, {"mc_samples": mc_samples, "mc_accuracy": None}, payload

    async def _handle_ip(
        self,
        json_obj: Dict[str, Any],
        program_context: str,
        determinism_runs: int,
        mc_samples: int,
        client: AsyncOpenAI,
        model: str,
        sampling_args: Optional[SamplingArgs],
        oai_tools: Optional[Any],
    ) -> Tuple[bool, Dict[str, Any], Dict[str, Any]]:
        message = json_obj.get("message", None)
        inputs = json_obj.get("inputs", None)
        payload = {"program": program_context, "input": None, "output": None, "message": message, "io_pairs": None, "visible_pairs": None, "hidden_pairs": None}
        if not isinstance(message, str):
            return False, {"mc_samples": mc_samples, "mc_accuracy": None, "error": "Missing or non-string 'message'"}, payload
        if not isinstance(inputs, list) or len(inputs) == 0:
            return False, {"mc_samples": mc_samples, "mc_accuracy": None, "error": "'inputs' must be a non-empty list"}, payload
        # Validate inputs against program: compute outputs
        f, err = self.executor.compile_program(program_context)
        if f is None:
            # Log compile error
            try:
                run_logger = _ensure_run_logger()
                run_logger.error(json.dumps({
                    "phase": "propose_compile_error",
                    "task": "induction.propose",
                    "error": err,
                    "program_preview": (program_context[:200] if isinstance(program_context, str) else None),
                }))
            except Exception:
                pass
            return False, {"mc_samples": mc_samples, "mc_accuracy": None, "error": f"Compile error: {err}"}, payload
        # Normalize each input: accept Python/JSON literals and multi-arg without parens
        normalized_inputs: List[Any] = []
        for raw_inp in inputs:
            if isinstance(raw_inp, str):
                s = raw_inp.strip()
                parsed: Any = None
                parsed_ok = False
                # Try JSON first
                try:
                    parsed = json.loads(s)
                    parsed_ok = True
                except Exception:
                    pass
                # Try Python literal_eval next
                if not parsed_ok:
                    try:
                        parsed = ast.literal_eval(s)
                        parsed_ok = True
                    except Exception:
                        pass
                # Try tuple-wrapping for multi-arg like "1, 'a', {'k':2}"
                if not parsed_ok and "," in s and not s.startswith(("[", "{", "(", "'", '"')):
                    try:
                        parsed = ast.literal_eval(f"({s})")
                        parsed_ok = True
                    except Exception:
                        pass
                if not parsed_ok:
                    # Log input parsing error
                    try:
                        run_logger = _ensure_run_logger()
                        run_logger.error(json.dumps({
                            "phase": "propose_input_parse_error",
                            "task": "induction.propose",
                            "error": f"Invalid input literal",
                            "literal_preview": s[:120],
                        }))
                    except Exception:
                        pass
                    return False, {"mc_samples": mc_samples, "mc_accuracy": None, "error": f"Invalid input literal: {s[:120]}"}, payload
                normalized_inputs.append(parsed)
            else:
                normalized_inputs.append(raw_inp)

        # Attempt to coerce flat tuples into a list of (name, info) pairs for single-arg functions
        # This helps when the proposer emits "'Sammy', {..}" or "'A',{..},'B',{..}" instead of
        # wrapping them as [(name, info), ...].
        try:
            sig = inspect.signature(f)
            if len(sig.parameters) == 1:
                coerced_inputs: List[Any] = []
                for parsed_in in normalized_inputs:
                    coerced_val: Any = parsed_in
                    if isinstance(parsed_in, tuple):
                        # Case: a single pair like ('Sammy', {...}) -> [(..., ...)]
                        if len(parsed_in) == 2 and not (isinstance(parsed_in[0], (list, tuple)) and len(parsed_in[0]) == 2):
                            coerced_val = [(parsed_in[0], parsed_in[1])]
                        # Case: flat even-length tuple like ('A', {...}, 'B', {...}) -> [(A,{...}), (B,{...})]
                        elif len(parsed_in) >= 2 and len(parsed_in) % 2 == 0:
                            if not all(isinstance(el, (list, tuple)) and len(el) == 2 for el in parsed_in):
                                pairs: List[Tuple[Any, Any]] = []
                                it = iter(parsed_in)
                                try:
                                    while True:
                                        first = next(it)
                                        second = next(it)
                                        pairs.append((first, second))
                                except StopIteration:
                                    pass
                                coerced_val = pairs
                    coerced_inputs.append(coerced_val)
                normalized_inputs = coerced_inputs
        except Exception:
            pass
        io_pairs: List[Tuple[Any, Any]] = []
        failed: List[Tuple[Any, Any]] = []
        for inp in normalized_inputs:
            ok, out, exec_err = self.executor.run_deterministic(f, inp, runs=determinism_runs)
            if ok:
                io_pairs.append((inp, out))
            else:
                failed.append((inp, exec_err))
        if not io_pairs:
            first_err = failed[0][1] if failed else "no valid inputs"
            bad_preview = repr(failed[0][0])[:80] if failed else ""
            # Log execution error for induction inputs
            try:
                run_logger = _ensure_run_logger()
                run_logger.error(json.dumps({
                    "phase": "propose_exec_error",
                    "task": "induction.propose",
                    "error": first_err,
                    "input_preview": bad_preview,
                    "program_preview": (program_context[:200] if isinstance(program_context, str) else None),
                }))
            except Exception:
                pass
            return False, {"mc_samples": mc_samples, "mc_accuracy": None, "error": f"All proposed inputs failed. Example: input {bad_preview} -> {first_err}"}, payload
        # Split visible/hidden halves
        random.shuffle(io_pairs)
        mid = max(1, len(io_pairs) // 2)
        visible = io_pairs[:mid]
        hidden = io_pairs[mid:]
        if not hidden:
            # ensure at least one hidden example
            hidden = visible[1:]
            visible = visible[:1]
        payload["io_pairs"] = io_pairs
        payload["visible_pairs"] = visible
        payload["hidden_pairs"] = hidden
        # Add to induction buffer
        step_id = self.buffers.add_induction(program=program_context, message=message, io_pairs=io_pairs, visible=visible, hidden=hidden)
        try:
            self.logger.info(
                f"[PROPOSE OK] induction.propose step_id={step_id} message_preview={(message or '')[:50]!r} visible={len(visible)} hidden={len(hidden)} total_induction={len(self.buffers.induction)}"
            )
        except Exception:
            pass
        # Defer MC solver accuracy to rubric stage
        return True, {"mc_samples": mc_samples, "mc_accuracy": None}, payload

    # -------- Solve handlers --------

    async def _handle_ds(self, json_obj: Dict[str, Any], item: Optional[DeductionItem], determinism_runs: int, raw_text: Optional[str] = None) -> Dict[str, Any]:
        output = json_obj.get("output", None)
        # Normalize literal types when represented as strings
        if isinstance(output, str):
            ok, val = self.rubric_impl._parse_literal_maybe_tuple(output)
            if ok:
                output = val
        payload = {
            "program": getattr(item, "program", None),
            "input": getattr(item, "input", None),
            "output": output,
            "gold_output": getattr(item, "output", None),
            "message": None,
            "io_pairs": None,
            "visible_pairs": None,
            "hidden_pairs": None,
        }
        return payload

    async def _handle_as(self, json_obj: Dict[str, Any], item: Optional[AbductionItem], determinism_runs: int, raw_text: Optional[str] = None) -> Dict[str, Any]:
        pred_input = json_obj.get("input", None)
        # Normalize literal types when represented as strings
        if isinstance(pred_input, str):
            ok, val = self.rubric_impl._parse_literal_maybe_tuple(pred_input)
            if ok:
                pred_input = val
        payload = {
            "program": getattr(item, "program", None),
            "input": pred_input,
            "output": getattr(item, "output", None),
            "gold_output": getattr(item, "output", None),
            "message": None,
            "io_pairs": None,
            "visible_pairs": None,
            "hidden_pairs": None,
        }
        return payload

    async def _handle_is(self, json_obj: Dict[str, Any], item: Optional[InductionItem], determinism_runs: int, raw_text: Optional[str] = None) -> Dict[str, Any]:
        program = json_obj.get("program", None)
        payload = {
            "program": program,
            "input": None,
            "output": None,
            "message": getattr(item, "message", None),
            "io_pairs": getattr(item, "io_pairs", None),
            "visible_pairs": getattr(item, "visible_pairs", None),
            "hidden_pairs": getattr(item, "hidden_pairs", None),
        }
        return payload


# =========================
# load_environment
# =========================

def _make_azr_dataset(
    system_prompt: str,
    dataset_repeats: int = 1000,
    K: int = 6,
    N: int = 10,
    mc_samples: int = 8,
    determinism_runs: int = 2,
) -> Dataset:
    """
    Construct a 6-row dataset (optionally repeated) with preformatted prompts (system message only).
    Dynamic content is injected in AZREnv.rollout.
    """
    base_prompt = [{"role": "system", "content": system_prompt}]
    rows = []
    tasks = [
        "deduction.propose",
        "abduction.propose",
        "induction.propose",
        "deduction.solve",
        "abduction.solve",
        "induction.solve",
    ]
    for _ in range(dataset_repeats):
        for t in tasks:
            rows.append(
                {
                    "prompt": deepcopy(base_prompt),
                    "answer": "",
                    "task": t,
                    "info": {"K": K, "N": N, "mc_samples": mc_samples, "determinism_runs": determinism_runs},
                }
            )
    return Dataset.from_list(rows)


def load_environment(
    # number of monte carlo samples to use for propose tasks (getting rewards)
    mc_samples: int = 6,
    # number of references to use for propose tasks
    proposer_K: int = 6,
    # number of inputs to ask for in induction propose tasks
    N: int = 10,
    determinism_runs: int = 2,
    seed: int = 1337420,
    system_prompt: str = BASE_SYSTEM_PROMPT,
    init_zero_triplet: bool = True,
    dataset_repeats: int = 1000,
    # Verbose printing control
    verbose: bool = False,
    exec_timeout: float = 20.0,
) -> vf.Environment:
    """
    Factory returning an AZREnv instance.
    """
    dataset = _make_azr_dataset(
        system_prompt=system_prompt,
        dataset_repeats=dataset_repeats,
        K=proposer_K,
        N=N,
        mc_samples=mc_samples,
        determinism_runs=determinism_runs,
    )
    env = AZREnv(
        dataset=dataset,
        system_prompt=system_prompt,
        mc_samples=mc_samples,
        proposer_K=proposer_K,
        N=N,
        determinism_runs=determinism_runs,
        seed=seed,
        init_zero_triplet=init_zero_triplet,
        verbose=verbose,
        exec_timeout=exec_timeout,
    )
    return env
